# -*- coding: utf-8 -*-
"""
Hanoi Coverage Report Generator

Usage:
    python hn_coverage_report.py --input "/path/to/input.xlsx" --output "/path/to/output.xlsx"

Requirements:
    - Python 3.8+
    - pandas
    - xlsxwriter

What it does:
    - Reads all sheets from the input Excel
    - Heuristically detects address and building-name columns
    - Flags rows in Hà Nội
    - Extracts quận/huyện using both district keywords and a ward→district dictionary
    - De-duplicates entries by (building_name,address)
    - Exports an Excel report with Summary, DistrictCoverage, RawHanoi

Author: Generated by ChatGPT
"""

import argparse
import pandas as pd
import re
import unicodedata
from datetime import datetime
from typing import Optional, Dict, List

# ----------------------------
# Text normalization helpers
# ----------------------------
def strip_accents(s: Optional[str]) -> str:
    if s is None or (isinstance(s, float) and pd.isna(s)):
        return ""
    s = str(s)
    s = unicodedata.normalize('NFD', s)
    s = ''.join(ch for ch in s if not unicodedata.combining(ch))
    s = unicodedata.normalize('NFC', s)
    return s

def norm_text(s: Optional[str]) -> str:
    s = strip_accents(s).lower()
    s = re.sub(r'[\s,;|/]+', ' ', s).strip()
    return s

# ----------------------------
# Column picking heuristics
# ----------------------------
ADDRESS_CANDIDATES_KW = ['địa', 'dia', 'address', 'addr', 'add', 'địa', 'dia chi', 'dia chi', 'location', 'địa chỉ', 'dia_chi', 'diachi']
NAME_CANDIDATES_KW    = ['tòa', 'toa', 'building', 'ten', 'tên', 'site', 'location name', 'property', 'mall', 'project', 'tower', 'asset', 'name']

def pick_col(cols: List[str], candidates_kw: List[str]) -> Optional[str]:
    scores = {}
    for c in cols:
        c_norm = norm_text(c)
        score = 0
        for kw in candidates_kw:
            if kw in c_norm:
                score += len(kw)
        score -= abs(len(c_norm)-10)*0.01
        scores[c] = score
    best = sorted(scores.items(), key=lambda x: x[1], reverse=True)
    if best and best[0][1] > 0:
        return best[0][0]
    return None

# ----------------------------
# Hanoi admin units & wards
# ----------------------------
HANOI_UNITS = [
    # Urban districts
    "Ba Đình","Hoàn Kiếm","Hai Bà Trưng","Đống Đa","Tây Hồ","Cầu Giấy","Thanh Xuân","Hoàng Mai","Long Biên","Hà Đông","Bắc Từ Liêm","Nam Từ Liêm",
    # Town
    "Sơn Tây",
    # Rural districts
    "Đông Anh","Gia Lâm","Thanh Trì","Thường Tín","Phú Xuyên","Ứng Hòa","Mỹ Đức","Phúc Thọ","Đan Phượng","Hoài Đức","Quốc Oai","Thạch Thất","Chương Mỹ","Thanh Oai","Ba Vì","Sóc Sơn","Mê Linh"
]

UNITS_NORM = [norm_text(u) for u in HANOI_UNITS]
UNIT_MAP = dict(zip(UNITS_NORM, HANOI_UNITS))

HANOI_TOKENS = ['ha noi','hanoi','tp ha noi','thanh pho ha noi'] + UNITS_NORM

# Core inner-district ward dictionary (can be expanded later)
DISTRICT_WARDS_TEXT = {
"Ba Đình": """
Cong Vi, Dien Bien, Doi Can, Giang Vo, Kim Ma, Lieu Giai, Ngoc Ha, Ngoc Khanh, Nguyen Trung Truc, Phuc Xa, Quan Thanh, Thanh Cong, Truc Bach, Vinh Phuc
""",
"Hoàn Kiếm": """
Chuong Duong, Cua Dong, Cua Nam, Dong Xuan, Hang Bac, Hang Bai, Hang Bo, Hang Bong, Hang Buom, Hang Gai, Hang Ma, Hang Trong, Ly Thai To, Phan Chu Trinh, Phuc Tan, Tran Hung Dao, Trang Tien, Hang Dao
""",
"Hai Bà Trưng": """
Bach Khoa, Bach Dang, Bui Thi Xuan, Cau Den, Dong Nhan, Dong Mac, Le Dai Hanh, Minh Khai, Nguyen Du, Pham Dinh Ho, Pho Hue, Quynh Loi, Quynh Mai, Thanh Luong, Thanh Nhan, Truong Dinh, Vinh Tuy
""",
"Đống Đa": """
Cat Linh, Hang Bot, Kham Thien, Kim Lien, Khuong Thuong, Lang Ha, Lang Thuong, Nam Dong, Nga Tu So, O Cho Dua, Phuong Lien, Phuong Mai, Quang Trung, Quoc Tu Giam, Thinh Quang, Tho Quan, Trung Liet, Trung Phung, Trung Tu, Van Chuong, Van Mieu
""",
"Tây Hồ": """
Buoi, Nhat Tan, Phu Thuong, Quang An, Thuy Khue, Tu Lien, Xuan La, Yen Phu
""",
"Cầu Giấy": """
Dich Vong, Dich Vong Hau, Mai Dich, Nghia Do, Nghia Tan, Quan Hoa, Trung Hoa, Yen Hoa
""",
"Thanh Xuân": """
Ha Dinh, Khuong Dinh, Khuong Mai, Khuong Trung, Nhan Chinh, Phuong Liet, Thanh Xuan Bac, Thanh Xuan Nam, Thanh Xuan Trung, Thuong Dinh
""",
"Hoàng Mai": """
Dai Kim, Dinh Cong, Giap Bat, Hoang Liet, Hoang Van Thu, Linh Nam, Mai Dong, Tan Mai, Thinh Liet, Tran Phu, Tuong Mai, Vinh Hung, Yen So
""",
"Long Biên": """
Bo De, Cu Khoi, Duc Giang, Gia Thuy, Giang Bien, Long Bien, Ngoc Lam, Ngoc Thuy, Phuc Dong, Phuc Loi, Sai Dong, Thach Ban, Thuong Thanh, Viet Hung
""",
"Hà Đông": """
Bien Giang, Duong Noi, Dong Mai, Ha Cau, Kien Hung, La Khe, Mo Lao, Nguyen Trai, Phu La, Phu Lam, Phu Luong, Phuc La, Quang Trung, Van Phuc, Van Quan, Yen Nghia, Yet Kieu
""",
"Nam Từ Liêm": """
Cau Dien, Dai Mo, Me Tri, My Dinh 1, My Dinh 2, Phuong Canh, Phu Do, Tay Mo, Trung Van, Xuan Phuong
""",
"Bắc Từ Liêm": """
Co Nhue 1, Co Nhue 2, Dong Ngac, Duc Thang, Lien Mac, Minh Khai, Phu Dien, Phuc Dien, Tay Tuu, Thuy Phuong, Xuan Dinh, Xuan Tao
""",
}

def build_ward_to_district() -> Dict[str, str]:
    ward_to_district_full = {}
    for dist, ward_str in DISTRICT_WARDS_TEXT.items():
        for w in re.split(r'[,;\n]+', ward_str):
            w = w.strip()
            if not w:
                continue
            ward_to_district_full[norm_text(w)] = dist
    # common aliases
    alias = {
        'o cho dua': 'Đống Đa',
        'tay mo': 'Nam Từ Liêm',
        'me tri': 'Nam Từ Liêm',
        'xuan phuong': 'Nam Từ Liêm',
        'dai mo': 'Nam Từ Liêm',
        'phuong canh': 'Nam Từ Liêm',
        'phu do': 'Nam Từ Liêm',
        'co nhue': 'Bắc Từ Liêm',
        'phuc dien': 'Bắc Từ Liêm',
        'dong ngac': 'Bắc Từ Liêm',
    }
    ward_to_district_full.update(alias)
    return ward_to_district_full

WARD_TO_DISTRICT = build_ward_to_district()

# ----------------------------
# Detection helpers
# ----------------------------
def detect_hanoi(addr: str) -> bool:
    t = norm_text(addr)
    return any(tok in t for tok in HANOI_TOKENS)

def extract_district(addr: str) -> Optional[str]:
    t = norm_text(addr)
    # direct unit keyword match
    for u_norm, u_vn in UNIT_MAP.items():
        if re.search(r'\b'+re.escape(u_norm)+r'\b', t):
            return u_vn
    # pattern-based
    m = re.search(r'\b(q|quan|h|huyen|tx|thi xa)\.?\s+([a-z0-9\s]+)', t)
    if m:
        candidate = m.group(2).strip()
        best = None
        best_score = 0
        for u_norm, u_vn in UNIT_MAP.items():
            score = sum(1 for tok in u_norm.split() if tok in candidate.split())
            if score > best_score:
                best_score = score
                best = u_vn
        if best and best_score>0:
            return best
    return None

def extract_district_with_wards(addr: str) -> Optional[str]:
    t = norm_text(addr)
    # 1) Try district detection
    d0 = extract_district(addr)
    if d0:
        return d0
    # 2) Ward lookup
    # check explicit 'phuong/p./xa/thi tran'
    m = re.search(r'\b(p|phuong|x|xa|thi tran|tt)\.?\s+([a-z0-9\s]+)', t)
    candidates = []
    if m:
        candidates.append(m.group(2).strip())
    # direct ward key presence
    for w_key, dist in WARD_TO_DISTRICT.items():
        if re.search(r'\b'+re.escape(w_key)+r'\b', t):
            return dist
    # fuzzy from candidate
    for cand in candidates:
        best = None
        best_score = 0
        for w_key, dist in WARD_TO_DISTRICT.items():
            toks = set(w_key.split())
            score = sum(1 for tok in toks if tok in cand.split())
            if score > best_score:
                best_score = score
                best = dist
        if best and best_score>0:
            return best
    return None

# ----------------------------
# Main processing
# ----------------------------
def load_all_sheets(input_path: str) -> pd.DataFrame:
    xls = pd.ExcelFile(input_path)
    frames = []
    for sheet_name in xls.sheet_names:
        df = xls.parse(sheet_name)
        df.columns = [str(c).strip() for c in df.columns]
        df2 = df.copy()
        df2['__sheet__'] = sheet_name

        addr_col = pick_col(df2.columns, ADDRESS_CANDIDATES_KW)
        name_col = pick_col(df2.columns, NAME_CANDIDATES_KW)

        if addr_col is None:
            # infer by value tokens
            for c in df2.columns:
                sample = ' '.join(map(str, df2[c].head(20).tolist()))
                sm = norm_text(sample)
                if any(tok in sm for tok in ['ha noi','hanoi','quan','huyen','tx','thi xa','phuong','xa','duong','pho']):
                    addr_col = c
                    break

        df2['__address_col__'] = addr_col or ""
        df2['__name_col__']    = name_col or ""
        df2['__address__']     = df2[addr_col].astype(str) if addr_col in df2.columns else ""
        df2['__name__']        = df2[name_col].astype(str) if name_col in df2.columns else ""
        frames.append(df2)

    if not frames:
        raise RuntimeError("Không đọc được dữ liệu từ file Excel.")
    return pd.concat(frames, ignore_index=True)

def pick_identifier(row: pd.Series) -> str:
    if isinstance(row['__name__'], str) and row['__name__'].strip():
        return row['__name__'].strip()
    for col in row.index:
        if col in ['__address__','__sheet__','__address_col__','__name_col__','__is_hanoi__','__district__']:
            continue
        v = row[col]
        if isinstance(v, str) and v.strip():
            s = norm_text(v)
            if not any(k in s for k in ['duong','pho','phuong','xa','quan','huyen','ha noi','hanoi','viet nam','vn']):
                return v
    return row['__address__']


def compute_report(df: pd.DataFrame) -> dict:
    df = df.copy()
    df['__is_hanoi__'] = df['__address__'].apply(detect_hanoi)
    df['__district__'] = df.apply(lambda r: extract_district_with_wards(r['__address__']) if r['__is_hanoi__'] else None, axis=1)
    df['__building_id__'] = df.apply(pick_identifier, axis=1)

    hanoi_df = df[df['__is_hanoi__']].copy()
    hanoi_df['__dedup_key__'] = hanoi_df['__building_id__'].astype(str).str.strip().str.lower() + '|' + hanoi_df['__address__'].astype(str).str.strip().str.lower()
    hanoi_df = hanoi_df.drop_duplicates(subset='__dedup_key__')

    # Raw counts including unknowns (None)
    district_counts_raw = hanoi_df['__district__'].value_counts(dropna=False)

    # Build a full index of all Hanoi district-level units + (optional) "Không rõ quận/huyện"
    full_index = list(HANOI_UNITS)
    # Add the "Không rõ quận/huyện" bucket if there are any unknowns
    unknown_label = 'Không rõ quận/huyện'
    if pd.isna(district_counts_raw.index).any() or (None in district_counts_raw.index if hasattr(district_counts_raw.index, '__contains__') else False):
        full_index = full_index + [unknown_label]

    # Map raw counts into a DataFrame with full_index and fill zeros
    # Convert district_counts_raw to a dict with proper label for NaN/None
    raw_map = {}
    for k, v in district_counts_raw.items():
        label = k if pd.notna(k) else unknown_label
        raw_map[label] = int(v)

    # Create DistrictCoverage table in required order and fill zeros for missing units
    district_counts = pd.DataFrame({
        'District': full_index,
        'Buildings': [raw_map.get(d, 0) for d in full_index]
    })

    num_districts_covered = sum(1 for d in HANOI_UNITS if raw_map.get(d, 0) > 0)
    total_admin_units = len(HANOI_UNITS)
    coverage_pct = round((num_districts_covered / total_admin_units * 100) if total_admin_units else 0.0, 2)

    # Summary table
    num_unknown = raw_map.get(unknown_label, 0)
    summary_df = pd.DataFrame([
        ['Tổng số dòng trong file', len(df)],
        ['Số địa điểm thuộc Hà Nội (sau khi loại trùng)', len(hanoi_df)],
        ['Số quận/huyện/thị xã có hiện diện', num_districts_covered],
        ['Tổng số đơn vị hành chính cấp huyện của Hà Nội (ước tính)', total_admin_units],
        ['Độ phủ theo đơn vị hành chính (%)', coverage_pct],
        ['Số toà “Không rõ quận/huyện”', int(num_unknown)],
    ], columns=['Chỉ tiêu','Giá trị'])

    # Export-ready RawHanoi with header "Quan/Phuong" (header only, values are district-level labels)
    export_df = hanoi_df[['__sheet__','__name__','__address__','__district__']].rename(columns={
        '__sheet__':'Sheet',
        '__name__':'Ten Toa nha (heuristic)',
        '__address__':'Dia chi',
        '__district__':'Quan/Phuong'
    }).copy()
    export_df['Quan/Phuong'] = export_df['Quan/Phuong'].fillna(unknown_label)

    return {
        'summary': summary_df,
        'district_counts': district_counts,
        'raw_hanoi': export_df
    }

def save_report(tables: dict, output_path: str) -> str:
    with pd.ExcelWriter(output_path, engine='xlsxwriter') as writer:
        tables['summary'].to_excel(writer, sheet_name='Summary', index=False)
        tables['district_counts'].to_excel(writer, sheet_name='DistrictCoverage', index=False)
        tables['raw_hanoi'].to_excel(writer, sheet_name='RawHanoi', index=False)
    return output_path

def main():
    parser = argparse.ArgumentParser(description="Generate Hanoi coverage report from LED building Excel.")
    parser.add_argument('--input', required=True, help='Path to input Excel file')
    parser.add_argument('--output', required=False, help='Path to output Excel report (.xlsx). If omitted, auto-named in same folder.')
    args = parser.parse_args()

    input_path = args.input
    output_path = args.output
    if not output_path:
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = re.sub(r'\.xlsx?$', '', input_path) + f'_HN_Coverage_{ts}.xlsx'

    df = load_all_sheets(input_path)
    tables = compute_report(df)
    save_report(tables, output_path)

    print("Report generated:", output_path)

if __name__ == '__main__':
    main()
