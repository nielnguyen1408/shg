# Seeding Knowledge Exporter (Auto-generate .md from approved_comments)

> File nÃ y Ä‘á»ƒ **Ä‘áº·t trong Knowledge**. Khi GPT cÃ³ **Code Interpreter + File Upload** báº­t, hÃ£y lÃ m Ä‘Ãºng quy trÃ¬nh dÆ°á»›i Ä‘Ã¢y Ä‘á»ƒ **Ä‘á»c `approved_comments.*` vÃ  xuáº¥t file giá»ng Ä‘iá»‡u (.md)**.

---

## CÃ¡ch dÃ¹ng nhanh
1) Upload `approved_comments.csv` hoáº·c `.json` vÃ o chat.
2) NÃ³i: **â€œXuáº¥t knowledge tÃªn: Seeding Ä‘iá»u hÆ°á»›ng brandingâ€** (hoáº·c tÃªn khÃ¡c).
3) Trá»£ lÃ½ pháº£i set biáº¿n tÃªn vÃ  **cháº¡y khá»‘i Python bÃªn dÆ°á»›i** khÃ´ng há»i láº¡i.
4) Táº£i file káº¿t quáº£, vÃ­ dá»¥: `seeding_dieu_huong_branding.md`, vÃ  upload vÃ o Knowledge cho cÃ¡c phiÃªn sau.

> Náº¿u báº¡n khÃ´ng Ä‘Æ°a tÃªn, máº·c Ä‘á»‹nh lÆ°u: `seeding_knowledge.md`.

---

## Quy táº¯c an toÃ n
- Chá»‰ dÃ¹ng cho seeding marketing há»£p phÃ¡p, ná»™i dung tÃ­ch cá»±c.
- KhÃ´ng PII, khÃ´ng nguyÃªn vÄƒn dÃ i; vÃ­ dá»¥ minh há»a â‰¤120 kÃ½ tá»±, rÃºt gá»n.
- KhÃ´ng kháº³ng Ä‘á»‹nh cháº¥t lÆ°á»£ng ngoÃ i pháº¡m vi vÄƒn phong.

---

## Khá»‘i PYTHON pháº£i cháº¡y (nguyÃªn khá»‘i, khÃ´ng dÃ¹ng internet)

```python
import os, re, json, math, unicodedata
import pandas as pd
from pathlib import Path
from collections import Counter

def to_stub(s):
    s = ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c)!='Mn')
    s = re.sub(r'[^a-zA-Z0-9]+','_', s.strip().lower()).strip('_')
    return s or "seeding_knowledge"

# Detect input file (Æ°u tiÃªn tÃªn chá»©a 'approved')
cands = [p for p in Path('.').glob('*') if p.is_file() and p.suffix.lower() in ('.csv','.json')]
if not cands:
    raise SystemExit("KhÃ´ng tháº¥y file CSV/JSON. HÃ£y upload 'approved_comments' trÆ°á»›c.")
target = sorted(cands, key=lambda p: ('approved' not in p.name.lower(), p.name))[0]

# Read to DataFrame
if target.suffix.lower()=='.csv':
    df = pd.read_csv(target)
else:
    df = pd.DataFrame(json.load(open(target, 'r', encoding='utf-8')))

if 'content' not in df.columns:
    raise SystemExit("Thiáº¿u cá»™t 'content' trong dá»¯ liá»‡u.")

# Clean & sample
texts = [str(x).strip() for x in df['content'].fillna('') if str(x).strip()]
texts = list(dict.fromkeys(texts))[:2000]  # háº¡n má»©c an toÃ n

# Tokenize
def toks(s): 
    s = re.sub(r'\s+',' ',s.lower()).strip()
    return re.findall(r"[a-zA-ZÃ€-á»¹0-9_]+|[!?]", s)

STOP = set("tÃ´i toi báº¡n ban mÃ¬nh minh chÃºng ta lÃ  thÃ¬ vÃ  cá»§a má»™t nhá»¯ng Ä‘Ã£ Ä‘ang sáº½ trong trÃªn dÆ°á»›i nÃ y kia Ä‘Ã¢y áº¥y cÃ¡c cho khÃ´ng ko Ä‘Ã¢u ná»¯a khi náº¿u vÃ¬ nÃªn bá»‹ Ä‘Æ°á»£c tá»«".split())
docs = [[t for t in toks(x) if re.match(r"^[a-zA-ZÃ€-á»¹0-9_]+$",t) and t not in STOP] for x in texts]

# Metrics
avg_len = round(sum(len(toks(x)) for x in texts)/max(1,len(texts)),2)
exclam = sum(x.count('!') for x in texts)/max(1,len(texts))
quest  = sum(x.count('?') for x in texts)/max(1,len(texts))
emoji  = sum(len(re.findall(r"[ğŸ™‚ğŸ˜ŠğŸ˜ğŸ˜„ğŸ‘ğŸ”¥âœ¨â¤ï¸ğŸ‘ŒğŸ¤ğŸ™ŒğŸ‰]", x)) for x in texts)/max(1,len(texts))

# crude tf-idf-ish
from collections import Counter
dfreq = Counter()
for ws in docs: 
    for w in set(ws): dfreq[w]+=1
N=len(docs)
scores=Counter()
for ws in docs:
    tf=Counter(ws)
    L=len(ws) or 1
    for w,f in tf.items():
        scores[w]+= (f/L)*math.log(1+N/(1+dfreq[w]))
uni = [w for w,_ in scores.most_common(15)]

def ngrams(ws,n): 
    return [" ".join(ws[i:i+n]) for i in range(len(ws)-n+1)]
bg = Counter(w for ws in docs for w in ngrams(ws,2)).most_common(10)
tg = Counter(w for ws in docs for w in ngrams(ws,3)).most_common(8)

# Persona heuristics
persona=[]
persona.append("Giá»ng tá»± nhiÃªn, tÃ­ch cá»±c" if emoji+exclam>0.2 else "Trung tÃ­nh, tiáº¿t cháº¿")
persona.append("CÃ¢u ngáº¯nâ€“trung bÃ¬nh, dá»… Ä‘á»c" if avg_len<25 else "CÃ¢u trung bÃ¬nhâ€“dÃ i, cÃ³ diá»…n giáº£i")
if emoji>0.2: persona.append("DÃ¹ng emoji má»©c vá»«a")
if exclam>0.2: persona.append("Nháº¥n nhÃ¡ cáº£m thÃ¡n")
if quest>0.15: persona.append("CÃ³ cÃ¢u há»i gá»£i má»Ÿ")

# Patterns + examples
def patternize(s):
    s=re.sub(r"\s+"," ",s).strip()
    s=re.sub(r"\b(mÃ¬nh|tÃ´i|báº¡n|má»i ngÆ°á»i)\b","[xÆ°ng hÃ´]",s,flags=re.I)
    s=re.sub(r"\b(thÆ°Æ¡ng hiá»‡u|sáº£n pháº©m|dá»‹ch vá»¥)\b","[Ä‘á»‘i tÆ°á»£ng]",s,flags=re.I)
    s=re.sub(r"\b(ban Ä‘áº§u|lÃºc Ä‘áº§u)\b","[má»Ÿ hoÃ i nghi]",s,flags=re.I)
    s=re.sub(r"\b(nhÆ°ng|rá»“i)\b","[chuyá»ƒn Ã½]",s,flags=re.I)
    return s[:120]

patterns=[]
for s in texts:
    p=patternize(s)
    if 8<=len(p)<=120 and p not in patterns:
        patterns.append(p)
    if len(patterns)>=8: break

if not patterns:
    patterns=["[xÆ°ng hÃ´] tháº¥y [Ä‘á»‘i tÆ°á»£ng] khÃ¡ á»•n, tráº£i nghiá»‡m vÆ°á»£t ká»³ vá»ng.",
              "Ban Ä‘áº§u hÆ¡i lÄƒn tÄƒn [chuyá»ƒn Ã½] dÃ¹ng rá»“i má»›i hiá»ƒu vÃ¬ sao Ä‘Æ°á»£c khen."]

examples=[]
for s in texts:
    s=re.sub(r"\s+"," ",s).strip()
    s=s[:120]
    if len(s)>=15 and s not in examples:
        examples.append(s)
    if len(examples)>=10: break

# Naming from user-provided vars (assistant pháº£i set trÆ°á»›c khi cháº¡y náº¿u ngÆ°á»i dÃ¹ng Ä‘áº·t tÃªn)
try:
    title = user_name_title
    stub  = user_name_stub
except NameError:
    title = "Seeding knowledge"; stub="seeding_knowledge"

# Render .md
md = ["# "+title, "", "## Persona", ", ".join(persona), "", "## Tone patterns"]
md += [f"- {p}" for p in patterns]
md += ["", "## Keyword clusters",
       "- Unigram: "+", ".join(uni),
       "- Bigram: "+", ".join([w for w,_ in bg]),
       "- Trigram: "+", ".join([w for w,_ in tg]),
       "", "## Sentiment summary",
       f"- Äá»™ dÃ i TB (token): {avg_len}",
       f"- Emoji/cmt: {round(emoji,3)}",
       f"- Cáº£m thÃ¡n/cmt: {round(exclam,3)}",
       f"- CÃ¢u há»i/cmt: {round(quest,3)}",
       "", "## Style constraints",
       "- TrÃ¡nh PR thÃ´, so sÃ¡nh cÃ´ng kÃ­ch.",
       "- Æ¯u tiÃªn cÃ¢u ngáº¯n, tráº£i nghiá»‡m cÃ¡ nhÃ¢n.",
       "- Emoji á»Ÿ má»©c phÃ¹ há»£p bá»‘i cáº£nh.",
       "", "## Recommendations",
       "- Giá»¯ ngÃ´n ngá»¯ Ä‘á»i thÆ°á»ng, cÃ³ 'Ä‘á»™ tháº­t'.",
       "- DÃ¹ng chuyá»ƒn Ã½ â€œban Ä‘áº§uâ€¦ nhÆ°ngâ€¦â€ khi há»£p lÃ½.",
       "- Gá»£i cÃ¢u há»i nháº¹ Ä‘á»ƒ má»Ÿ tháº£o luáº­n.",
       "- Táº­n dá»¥ng tá»« khÃ³a chá»§ Ä‘áº¡o má»™t cÃ¡ch tá»± nhiÃªn.",
       "", "## Example lines"]
md += [f"- {e}" for e in examples]

out = Path(f"/mnt/data/{stub}.md")
out.write_text("\n".join(md), encoding="utf-8")
out
```

---

## Macro há»™i thoáº¡i
- â€œ**Xuáº¥t knowledge tÃªn: Seeding Ä‘iá»u hÆ°á»›ng branding**â€ â†’ trá»£ lÃ½ set:  
  `user_name_title = "Seeding Ä‘iá»u hÆ°á»›ng branding"`  
  `user_name_stub  = "seeding_dieu_huong_branding"`  
  rá»“i cháº¡y khá»‘i Python, tráº£ link:  
  `sandbox:/mnt/data/seeding_dieu_huong_branding.md`

- â€œ**Liá»‡t kÃª knowledge Ä‘Ã£ táº¡o**â€ â†’ liá»‡t kÃª táº¥t cáº£ file `.md` trong `/mnt/data`.

- â€œ**Gá»™p knowledge A + B thÃ nh C**â€ â†’ Ä‘á»c 2 file `.md`, há»£p nháº¥t Persona/Patterns/Keywords/Recommendations, lÆ°u `C.md`.


# --- JSON pháº³ng (khÃ´ng lá»“ng) ---
flat = {
    "title": title,
    "persona": "; ".join(persona),
    "tone_patterns": " | ".join(patterns),
    "unigram_keywords": ", ".join(uni),
    "bigram_keywords": ", ".join([w for w,_ in bg]),
    "trigram_keywords": ", ".join([w for w,_ in tg]),
    "avg_token_length": avg_len,
    "emoji_per_comment": round(emoji,3),
    "exclaim_per_comment": round(exclam,3),
    "question_per_comment": round(quest,3),
    "style_constraints": "TrÃ¡nh PR thÃ´; Æ¯u tiÃªn cÃ¢u ngáº¯n, tráº£i nghiá»‡m; Emoji vá»«a pháº£i",
    "recommendations": "Giá»¯ ngÃ´n ngá»¯ Ä‘á»i thÆ°á»ng; DÃ¹ng chuyá»ƒn Ã½ 'ban Ä‘áº§uâ€¦ nhÆ°ngâ€¦'; Gá»£i cÃ¢u há»i nháº¹; DÃ¹ng keyword tá»± nhiÃªn",
    "example_lines": " | ".join(examples)
}
json_path = Path(f"/mnt/data/{stub}_flat.json")
json_path.write_text(json.dumps(flat, ensure_ascii=False, indent=2), encoding="utf-8")

# --- Excel tá»•ng há»£p ---
excel_path = Path(f"/mnt/data/{stub}.xlsx")
with pd.ExcelWriter(excel_path, engine="xlsxwriter") as writer:
    pd.DataFrame([flat]).to_excel(writer, index=False, sheet_name="Summary")
    df.to_excel(writer, index=False, sheet_name="ApprovedComments")
    pd.DataFrame({"tone_patterns": patterns}).to_excel(writer, index=False, sheet_name="Patterns")
    pd.DataFrame({"unigram": uni}).to_excel(writer, index=False, sheet_name="Keywords-Uni")
    pd.DataFrame({"bigram": [w for w,_ in bg]}).to_excel(writer, index=False, sheet_name="Keywords-Bi")
    pd.DataFrame({"trigram": [w for w,_ in tg]}).to_excel(writer, index=False, sheet_name="Keywords-Tri")

import json, pandas as pd
from pathlib import Path
from datetime import datetime

# === INPUT giáº£ láº­p (GPT sáº½ thay báº±ng dá»¯ liá»‡u tháº­t tá»« ngÆ°á»i dÃ¹ng) ===
input_data = {
  "bai_viet_goc": "ThÆ°Æ¡ng hiá»‡u ABC vá»«a ra máº¯t dÃ²ng sáº£n pháº©m skincare má»›i...",
  "dinh_huong": ["Nháº¥n máº¡nh Ä‘á»™ an toÃ n", "Táº­p trung tráº£i nghiá»‡m thá»±c táº¿ ngÆ°á»i dÃ¹ng"],
  "tone": "tá»± nhiÃªn",
  "mood": "tÃ­ch cá»±c",
  "so_luong": 10,
  "do_dai_tb": 2,
  "yeu_cau_khac": "cÃ³ emoji, xen láº«n vÃ i bÃ¬nh luáº­n pháº£n há»“i qua láº¡i"
}

# === Sinh comment máº«u (GPT sinh tháº­t á»Ÿ runtime) ===
comments = [
  "DÃ²ng nÃ y mÃ¬nh dÃ¹ng thá»­ tháº¥y dá»‹u nháº¹ tháº­t, da nháº¡y cáº£m váº«n á»•n ğŸ˜Š",
  "Ban Ä‘áº§u khÃ´ng tin máº¥y quáº£ng cÃ¡o Ä‘Ã¢u, mÃ  dÃ¹ng rá»“i pháº£i cÃ´ng nháº­n cháº¥t lÆ°á»£ng tá»‘t.",
  "Chai thiáº¿t káº¿ xinh mÃ  mÃ¹i dá»… chá»‹u, Ä‘Ã¡ng Ä‘á»“ng tiá»n.",
  "Äá»c bÃ i nÃ y xong má»›i biáº¿t hÃ£ng cÃ³ quy trÃ¬nh kiá»ƒm Ä‘á»‹nh rÃµ rÃ ng váº­y luÃ´n ğŸ”¥",
  "Ai dÃ¹ng rá»“i chia sáº» thÃªm cáº£m nháº­n vá»›i, mÃ¬nh Ä‘ang cÃ¢n nháº¯c mua ğŸ˜…",
  "Tháº¥y ai review cÅ©ng khen, cháº¯c pháº£i thá»­ má»™t chai xem sao.",
  "Äiá»ƒm cá»™ng lÃ  khÃ´ng bá»‹ kÃ­ch á»©ng, da mÃ¬nh yáº¿u mÃ  váº«n dÃ¹ng Ä‘Æ°á»£c.",
  "ÄÃºng kiá»ƒu skincare dÃ nh cho ngÆ°á»i lÆ°á»i mÃ  váº«n muá»‘n Ä‘áº¹p ğŸ˜",
  "ThÆ°Æ¡ng hiá»‡u nÃ y lÃ m ná»™i dung lÃºc nÃ o cÅ©ng gáº§n gÅ©i, dá»… tin.",
  "Cáº£m Æ¡n bÃ i chia sáº», Ä‘á»c mÃ  muá»‘n chÄƒm sÃ³c báº£n thÃ¢n hÆ¡n luÃ´n!"
]

# === Táº¡o dá»¯ liá»‡u pháº³ng ===
flat_records = []
for i, c in enumerate(comments, 1):
    flat_records.append({
        "id": i,
        "comment": c,
        "tone": input_data["tone"],
        "mood": input_data["mood"],
        "source_post": input_data["bai_viet_goc"][:100] + "...",
        "dinh_huong": "; ".join(input_data["dinh_huong"]),
        "extra": input_data.get("yeu_cau_khac", "")
    })

# === Xuáº¥t JSON pháº³ng ===
timestamp = datetime.now().strftime("%Y%m%d_%H%M")
json_path = Path(f"/mnt/data/seeding_output_{timestamp}.json")
with open(json_path, "w", encoding="utf-8") as f:
    json.dump(flat_records, f, ensure_ascii=False, indent=2)

# === Xuáº¥t Excel ===
excel_path = Path(f"/mnt/data/seeding_output_{timestamp}.xlsx")
pd.DataFrame(flat_records).to_excel(excel_path, index=False, sheet_name="SeedingComments")

(json_path, excel_path)
