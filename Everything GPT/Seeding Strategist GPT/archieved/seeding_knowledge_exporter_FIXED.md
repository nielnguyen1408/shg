# Seeding Knowledge Exporter (Auto .md + flat JSON + Excel + Labeled Comments)
> Äáº·t file nÃ y trong **Knowledge**. Báº­t **Code Interpreter + File Upload**.
> Quy trÃ¬nh:
> 1) Upload `approved_comments.csv` hoáº·c `.json`
> 2) NÃ³i: **â€œXuáº¥t knowledge tÃªn: <TÃªn báº¡n muá»‘n>â€**
> 3) Trá»£ lÃ½ set biáº¿n tÃªn vÃ  **cháº¡y khá»‘i Python duy nháº¥t** á»Ÿ dÆ°á»›i.
> 4) Káº¿t quáº£: `<stub>.md` (Knowledge), `<stub>_flat.json` (summary pháº³ng), `<stub>.xlsx` (Ä‘áº§y Ä‘á»§), cá»™ng thÃªm **báº£ng gÃ¡n nhÃ£n tá»«ng comment** vá»›i 2 cá»™t má»›i: `muc_dich`, `chien_thuat`.
>
> Taxonomy máº·c Ä‘á»‹nh (cÃ³ thá»ƒ chá»‰nh trong code):
> - `muc_dich`: kÃ­ch hoáº¡t tháº£o luáº­n | xÃ¡c nháº­n uy tÃ­n | chia sáº» tráº£i nghiá»‡m | pháº£n biá»‡n nháº¹ | khuáº¿ch Ä‘áº¡i cáº£m xÃºc | chá»‘t háº¡ mua hÃ ng | Ä‘á»‹nh hÆ°á»›ng thÆ°Æ¡ng hiá»‡u | hÃ i hÆ°á»›c / giáº£i trÃ­
> - `chien_thuat`: há»i gá»£i má»Ÿ | cáº£m thÃ¡n tÃ­ch cá»±c | so sÃ¡nh nháº¹ | chuyá»ƒn Ã½ tá»± nhiÃªn | truyá»n cáº£m há»©ng | hÃ i hÆ°á»›c nháº¹ | Ä‘á»“ng cáº£m cÃ¡ nhÃ¢n | Ä‘á» xuáº¥t / khuyáº¿n nghá»‹

---

## KHá»I PYTHON DUY NHáº¤T (cháº¡y nguyÃªn khá»‘i, khÃ´ng internet)
```python
import os, re, json, math, unicodedata
import pandas as pd
from pathlib import Path
from collections import Counter

def to_stub(s):
    s = ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c)!='Mn')
    s = re.sub(r'[^a-zA-Z0-9]+','_', s.strip().lower()).strip('_')
    return s or "seeding_knowledge"

# Detect input file (Æ°u tiÃªn tÃªn chá»©a 'approved')
cands = [p for p in Path('.').glob('*') if p.is_file() and p.suffix.lower() in ('.csv','.json')]
if not cands:
    raise SystemExit("KhÃ´ng tháº¥y file CSV/JSON. HÃ£y upload 'approved_comments' trÆ°á»›c.")
target = sorted(cands, key=lambda p: ('approved' not in p.name.lower(), p.name))[0]

# Read to DataFrame
if target.suffix.lower()=='.csv':
    df = pd.read_csv(target)
else:
    df = pd.DataFrame(json.load(open(target, 'r', encoding='utf-8')))

if 'content' not in df.columns:
    raise SystemExit("Thiáº¿u cá»™t 'content' trong dá»¯ liá»‡u.")

# Clean & sample
texts = [str(x).strip() for x in df['content'].fillna('') if str(x).strip()]
texts = list(dict.fromkeys(texts))[:5000]  # háº¡n má»©c an toÃ n

# Tokenize
def toks(s): 
    s = re.sub(r'\s+',' ',s.lower()).strip()
    return re.findall(r"[a-zA-ZÃ€-á»¹0-9_]+|[!?]", s)

STOP = set("tÃ´i toi báº¡n ban mÃ¬nh minh chÃºng ta lÃ  thÃ¬ vÃ  cá»§a má»™t nhá»¯ng Ä‘Ã£ Ä‘ang sáº½ trong trÃªn dÆ°á»›i nÃ y kia Ä‘Ã¢y áº¥y cÃ¡c cho khÃ´ng ko Ä‘Ã¢u ná»¯a khi náº¿u vÃ¬ nÃªn bá»‹ Ä‘Æ°á»£c tá»«".split())
docs = [[t for t in toks(x) if re.match(r"^[a-zA-ZÃ€-á»¹0-9_]+$",t) and t not in STOP] for x in texts]

# Metrics
avg_len = round(sum(len(toks(x)) for x in texts)/max(1,len(texts)),2)
exclam = sum(x.count('!') for x in texts)/max(1,len(texts))
quest  = sum(x.count('?') for x in texts)/max(1,len(texts))
emoji  = sum(len(re.findall(r"[ğŸ™‚ğŸ˜ŠğŸ˜ğŸ˜„ğŸ‘ğŸ”¥âœ¨â¤ï¸ğŸ‘ŒğŸ¤ğŸ™ŒğŸ‰]", x)) for x in texts)/max(1,len(texts))

# crude tf-idf-ish
dfreq = Counter()
for ws in docs: 
    for w in set(ws): dfreq[w]+=1
N=len(docs)
scores=Counter()
for ws in docs:
    tf=Counter(ws)
    L=len(ws) or 1
    for w,f in tf.items():
        scores[w]+= (f/L)*math.log(1+N/(1+dfreq[w]))
uni = [w for w,_ in scores.most_common(15)]

def ngrams(ws,n): 
    return [" ".join(ws[i:i+n]) for i in range(len(ws)-n+1)]
bg = Counter(w for ws in docs for w in ngrams(ws,2)).most_common(10)
tg = Counter(w for ws in docs for w in ngrams(ws,3)).most_common(8)

# Persona heuristics
persona=[]
persona.append("Giá»ng tá»± nhiÃªn, tÃ­ch cá»±c" if emoji+exclam>0.2 else "Trung tÃ­nh, tiáº¿t cháº¿")
persona.append("CÃ¢u ngáº¯nâ€“trung bÃ¬nh, dá»… Ä‘á»c" if avg_len<25 else "CÃ¢u trung bÃ¬nhâ€“dÃ i, cÃ³ diá»…n giáº£i")
if emoji>0.2: persona.append("DÃ¹ng emoji má»©c vá»«a")
if exclam>0.2: persona.append("Nháº¥n nhÃ¡ cáº£m thÃ¡n")
if quest>0.15: persona.append("CÃ³ cÃ¢u há»i gá»£i má»Ÿ")

# Patterns + examples
def patternize(s):
    s=re.sub(r"\s+"," ",s).strip()
    s=re.sub(r"\b(mÃ¬nh|tÃ´i|báº¡n|má»i ngÆ°á»i)\b","[xÆ°ng hÃ´]",s,flags=re.I)
    s=re.sub(r"\b(thÆ°Æ¡ng hiá»‡u|sáº£n pháº©m|dá»‹ch vá»¥)\b","[Ä‘á»‘i tÆ°á»£ng]",s,flags=re.I)
    s=re.sub(r"\b(ban Ä‘áº§u|lÃºc Ä‘áº§u)\b","[má»Ÿ hoÃ i nghi]",s,flags=re.I)
    s=re.sub(r"\b(nhÆ°ng|rá»“i)\b","[chuyá»ƒn Ã½]",s,flags=re.I)
    return s[:120]

patterns=[]
for s in texts:
    p=patternize(s)
    if 8<=len(p)<=120 and p not in patterns:
        patterns.append(p)
    if len(patterns)>=8: break

if not patterns:
    patterns=["[xÆ°ng hÃ´] tháº¥y [Ä‘á»‘i tÆ°á»£ng] khÃ¡ á»•n, tráº£i nghiá»‡m vÆ°á»£t ká»³ vá»ng.",
              "Ban Ä‘áº§u hÆ¡i lÄƒn tÄƒn [chuyá»ƒn Ã½] dÃ¹ng rá»“i má»›i hiá»ƒu vÃ¬ sao Ä‘Æ°á»£c khen."]

examples=[]
for s in texts:
    s=re.sub(r"\s+"," ",s).strip()
    s=s[:120]
    if len(s)>=15 and s not in examples:
        examples.append(s)
    if len(examples)>=10: break

# ---------- Heuristic labeling for each original comment ----------
def label_purpose(text):
    t = text.lower()
    if "?" in t or any(k in t for k in ["ai dÃ¹ng", "cÃ³ ai", "khÃ´ng nhá»‰", "khÃ´ng?", "nhá»‰?"]):
        return "kÃ­ch hoáº¡t tháº£o luáº­n"
    if any(k in t for k in ["quy trÃ¬nh", "kiá»ƒm Ä‘á»‹nh", "chÃ­nh hÃ£ng", "báº£o hÃ nh", "chá»©ng nháº­n", "nguá»“n gá»‘c"]):
        return "xÃ¡c nháº­n uy tÃ­n"
    if any(k in t for k in ["ban Ä‘áº§u", "lÃºc Ä‘áº§u", "nghÄ©", "tÆ°á»Ÿng", "nhÆ°ng"]):
        return "pháº£n biá»‡n nháº¹"
    if any(k in t for k in ["mua", "Ä‘áº·t", "thá»­", "pháº£i thá»­", "chá»‘t", "rinh"]):
        return "chá»‘t háº¡ mua hÃ ng"
    if any(emo in t for emo in ["ğŸ˜Š","ğŸ˜","ğŸ˜„","ğŸ‘","ğŸ”¥","âœ¨","â¤ï¸","ğŸ‘Œ","ğŸ¤","ğŸ™Œ","ğŸ‰"]):
        return "khuáº¿ch Ä‘áº¡i cáº£m xÃºc"
    if any(k in t for k in ["tráº£i nghiá»‡m", "dÃ¹ng rá»“i", "cáº£m nháº­n", "review", "xÃ i rá»“i"]):
        return "chia sáº» tráº£i nghiá»‡m"
    if any(k in t for k in ["cÃ¢u chuyá»‡n", "giÃ¡ trá»‹", "thÃ´ng Ä‘iá»‡p", "thÆ°Æ¡ng hiá»‡u", "tinh tháº§n"]):
        return "Ä‘á»‹nh hÆ°á»›ng thÆ°Æ¡ng hiá»‡u"
    return "hÃ i hÆ°á»›c / giáº£i trÃ­" if any(k in t for k in ["haha", "hihi", "vui", "cÆ°á»i"]) else "chia sáº» tráº£i nghiá»‡m"

def label_tactic(text):
    t = text.lower()
    if "?" in t:
        return "há»i gá»£i má»Ÿ"
    if any(k in t for k in ["ban Ä‘áº§u", "lÃºc Ä‘áº§u", "nhÆ°ng"]):
        return "chuyá»ƒn Ã½ tá»± nhiÃªn"
    if any(emo in t for emo in ["ğŸ˜Š","ğŸ˜","ğŸ˜„","ğŸ‘","ğŸ”¥","âœ¨","â¤ï¸","ğŸ‘Œ","ğŸ¤","ğŸ™Œ","ğŸ‰","!"]):
        return "cáº£m thÃ¡n tÃ­ch cá»±c"
    if any(k in t for k in ["so vá»›i", "giá»‘ng nhÆ°", "kiá»ƒu nhÆ°"]):
        return "so sÃ¡nh nháº¹"
    if any(k in t for k in ["truyá»n cáº£m há»©ng", "Ä‘á»™ng lá»±c", "lan tá»a"]):
        return "truyá»n cáº£m há»©ng"
    if any(k in t for k in ["haha", "hihi", "ğŸ˜…", "ğŸ˜"]):
        return "hÃ i hÆ°á»›c nháº¹"
    if any(k in t for k in ["mÃ¬nh tháº¥y", "theo mÃ¬nh", "cÃ¡ nhÃ¢n", "tráº£i nghiá»‡m"]):
        return "Ä‘á»“ng cáº£m cÃ¡ nhÃ¢n"
    if any(k in t for k in ["nÃªn", "thá»­", "xem", "cÃ¢n nháº¯c"]):
        return "Ä‘á» xuáº¥t / khuyáº¿n nghá»‹"
    return "Ä‘á»“ng cáº£m cÃ¡ nhÃ¢n"

labeled = []
for i, c in enumerate(texts, 1):
    labeled.append({
        "id": i,
        "content": c,
        "muc_dich": label_purpose(c),
        "chien_thuat": label_tactic(c)
    })

# Naming from user-provided vars (assistant pháº£i set trÆ°á»›c khi cháº¡y náº¿u ngÆ°á»i dÃ¹ng Ä‘áº·t tÃªn)
try:
    title = user_name_title
    stub  = user_name_stub
except NameError:
    title = "Seeding knowledge"; stub="seeding_knowledge"

# --------- Render & SAVE .md ---------
md = ["# "+title, "", "## Persona", ", ".join(persona), "", "## Tone patterns"]
md += [f"- {p}" for p in patterns]
md += ["", "## Keyword clusters",
       "- Unigram: "+", ".join(uni),
       "- Bigram: "+", ".join([w for w,_ in bg]),
       "- Trigram: "+", ".join([w for w,_ in tg]),
       "", "## Sentiment summary",
       f"- Äá»™ dÃ i TB (token): {avg_len}",
       f"- Emoji/cmt: {round(emoji,3)}",
       f"- Cáº£m thÃ¡n/cmt: {round(exclam,3)}",
       f"- CÃ¢u há»i/cmt: {round(quest,3)}",
       "", "## Style constraints",
       "- TrÃ¡nh PR thÃ´, so sÃ¡nh cÃ´ng kÃ­ch.",
       "- Æ¯u tiÃªn cÃ¢u ngáº¯n, tráº£i nghiá»‡m cÃ¡ nhÃ¢n.",
       "- Emoji á»Ÿ má»©c phÃ¹ há»£p bá»‘i cáº£nh.",
       "", "## Recommendations",
       "- Giá»¯ ngÃ´n ngá»¯ Ä‘á»i thÆ°á»ng, cÃ³ 'Ä‘á»™ tháº­t'.",
       "- DÃ¹ng chuyá»ƒn Ã½ â€œban Ä‘áº§uâ€¦ nhÆ°ngâ€¦â€ khi há»£p lÃ½.",
       "- Gá»£i cÃ¢u há»i nháº¹ Ä‘á»ƒ má»Ÿ tháº£o luáº­n.",
       "- Táº­n dá»¥ng tá»« khÃ³a chá»§ Ä‘áº¡o má»™t cÃ¡ch tá»± nhiÃªn.",
       "", "## Example lines"]
md += [f"- {e}" for e in examples]

md_path = Path(f"/mnt/data/{stub}.md")
md_path.write_text("\n".join(md), encoding="utf-8")

# --------- SAVE flat JSON (summary, no nesting) ---------
flat = {
    "title": title,
    "persona": "; ".join(persona),
    "tone_patterns": " | ".join(patterns),
    "unigram_keywords": ", ".join(uni),
    "bigram_keywords": ", ".join([w for w,_ in bg]),
    "trigram_keywords": ", ".join([w for w,_ in tg]),
    "avg_token_length": avg_len,
    "emoji_per_comment": round(emoji,3),
    "exclaim_per_comment": round(exclam,3),
    "question_per_comment": round(quest,3),
    "style_constraints": "TrÃ¡nh PR thÃ´; Æ¯u tiÃªn cÃ¢u ngáº¯n, tráº£i nghiá»‡m; Emoji vá»«a pháº£i",
    "recommendations": "Giá»¯ ngÃ´n ngá»¯ Ä‘á»i thÆ°á»ng; DÃ¹ng chuyá»ƒn Ã½ 'ban Ä‘áº§uâ€¦ nhÆ°ngâ€¦'; Gá»£i cÃ¢u há»i nháº¹; DÃ¹ng keyword tá»± nhiÃªn",
    "example_lines": " | ".join(examples)
}
json_path = Path(f"/mnt/data/{stub}_flat.json")
json_path.write_text(json.dumps(flat, ensure_ascii=False, indent=2), encoding="utf-8")

# --------- SAVE Excel (thÃªm sheet LabeledComments) ---------
excel_path = Path(f"/mnt/data/{stub}.xlsx")
with pd.ExcelWriter(excel_path, engine="xlsxwriter") as writer:
    pd.DataFrame([flat]).to_excel(writer, index=False, sheet_name="Summary")
    df.to_excel(writer, index=False, sheet_name="ApprovedComments")
    pd.DataFrame({"tone_patterns": patterns}).to_excel(writer, index=False, sheet_name="Patterns")
    pd.DataFrame({"unigram": uni}).to_excel(writer, index=False, sheet_name="Keywords-Uni")
    pd.DataFrame({"bigram": [w for w,_ in bg]}).to_excel(writer, index=False, sheet_name="Keywords-Bi")
    pd.DataFrame({"trigram": [w for w,_ in tg]}).to_excel(writer, index=False, sheet_name="Keywords-Tri")
    pd.DataFrame(labeled).to_excel(writer, index=False, sheet_name="LabeledComments")

(md_path, json_path, excel_path)
```

---

### Ghi chÃº triá»ƒn khai
- Hai cá»™t má»›i (`muc_dich`, `chien_thuat`) Ä‘Ã£ Ä‘Æ°á»£c sinh tá»± Ä‘á»™ng vÃ  xuáº¥t vÃ o **sheet `LabeledComments`** cá»§a Excel.
- Báº¡n **khÃ´ng cáº§n** táº¡o file schema riÃªng cho hai cá»™t nÃ y, trá»« khi team muá»‘n chá»‰nh taxonomy thÆ°á»ng xuyÃªn. Náº¿u cáº§n, táº¡o thÃªm má»™t file `taxonomy_seeding.md` trong Knowledge ghi rÃµ danh má»¥c vÃ  quy táº¯c gÃ¡n â€” cÃ²n logic gÃ¡n thÃ¬ **giá»¯ trong code** Ä‘á»ƒ báº£o Ä‘áº£m nháº¥t quÃ¡n.
