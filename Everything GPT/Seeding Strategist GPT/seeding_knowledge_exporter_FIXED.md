# Seeding Knowledge Exporter (Auto .md + flat JSON + Excel + Labeled Comments + Ideation)
> Äáº·t file nÃ y trong **Knowledge**. Báº­t **Code Interpreter + File Upload**.
> Quy trÃ¬nh:
> 1) Upload `approved_comments.csv` hoáº·c `.json`
> 2) NÃ³i: **â€œXuáº¥t knowledge tÃªn: <TÃªn báº¡n muá»‘n>â€**
> 3) Trá»£ lÃ½ set biáº¿n tÃªn vÃ  **cháº¡y khá»‘i Python duy nháº¥t** á»Ÿ dÆ°á»›i.
> 4) Káº¿t quáº£: `<stub>.md` (Knowledge), `<stub>_flat.json` (summary pháº³ng), `<stub>.xlsx` (Ä‘áº§y Ä‘á»§), `*_ideas.json` vÃ  sheet `Ideas`. Duyá»‡t `approved_ideas=[...]` rá»“i cháº¡y láº¡i Ä‘á»ƒ sinh `SeedingComments`.

---

## KHá»I PYTHON DUY NHáº¤T (cháº¡y nguyÃªn khá»‘i, khÃ´ng internet)
```python
import os, re, json, math, unicodedata, random, hashlib
import pandas as pd
from pathlib import Path
from collections import Counter

def to_stub(s):
    s = ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c)!='Mn')
    s = re.sub(r'[^a-zA-Z0-9]+','_', s.strip().lower()).strip('_')
    return s or "seeding_knowledge"

# Detect input file (Æ°u tiÃªn tÃªn chá»©a 'approved')
cands = [p for p in Path('.').glob('*') if p.is_file() and p.suffix.lower() in ('.csv','.json')]
if not cands:
    raise SystemExit("KhÃ´ng tháº¥y file CSV/JSON. HÃ£y upload 'approved_comments' trÆ°á»›c.")
target = sorted(cands, key=lambda p: ('approved' not in p.name.lower(), p.name))[0]

# Read to DataFrame
if target.suffix.lower()=='.csv':
    df = pd.read_csv(target)
else:
    df = pd.DataFrame(json.load(open(target, 'r', encoding='utf-8')))

if 'content' not in df.columns:
    raise SystemExit("Thiáº¿u cá»™t 'content' trong dá»¯ liá»‡u.")

# Clean & sample
texts = [str(x).strip() for x in df['content'].fillna('') if str(x).strip()]
texts = list(dict.fromkeys(texts))[:5000]

# Tokenize
def toks(s): 
    s = re.sub(r'\s+',' ',s.lower()).strip()
    return re.findall(r"[a-zA-ZÃ€-á»¹0-9_]+|[!?]", s)

STOP = set("tÃ´i toi báº¡n ban mÃ¬nh minh chÃºng ta lÃ  thÃ¬ vÃ  cá»§a má»™t nhá»¯ng Ä‘Ã£ Ä‘ang sáº½ trong trÃªn dÆ°á»›i nÃ y kia Ä‘Ã¢y áº¥y cÃ¡c cho khÃ´ng ko Ä‘Ã¢u ná»¯a khi náº¿u vÃ¬ nÃªn bá»‹ Ä‘Æ°á»£c tá»«".split())
docs = [[t for t in toks(x) if re.match(r"^[a-zA-ZÃ€-á»¹0-9_]+$",t) and t not in STOP] for x in texts]

# Metrics
avg_len = round(sum(len(toks(x)) for x in texts)/max(1,len(texts)),2)
exclam = sum(x.count('!') for x in texts)/max(1,len(texts))
quest  = sum(x.count('?') for x in texts)/max(1,len(texts))
emoji  = sum(len(re.findall(r"[ğŸ™‚ğŸ˜ŠğŸ˜ğŸ˜„ğŸ‘ğŸ”¥âœ¨â¤ï¸ğŸ‘ŒğŸ¤ğŸ™ŒğŸ‰]", x)) for x in texts)/max(1,len(texts))

# crude tf-idf-ish
dfreq = Counter()
for ws in docs: 
    for w in set(ws): dfreq[w]+=1
N=len(docs)
scores=Counter()
for ws in docs:
    tf=Counter(ws)
    L=len(ws) or 1
    for w,f in tf.items():
        scores[w]+= (f/L)*math.log(1+N/(1+dfreq[w]))
uni = [w for w,_ in scores.most_common(15)]

def ngrams(ws,n): 
    return [" ".join(ws[i:i+n]) for i in range(len(ws)-n+1)]
bg = Counter(w for ws in docs for w in ngrams(ws,2)).most_common(10)
tg = Counter(w for ws in docs for w in ngrams(ws,3)).most_common(8)

# Persona heuristics
persona=[]
persona.append("Giá»ng tá»± nhiÃªn, tÃ­ch cá»±c" if emoji+exclam>0.2 else "Trung tÃ­nh, tiáº¿t cháº¿")
persona.append("CÃ¢u ngáº¯nâ€“trung bÃ¬nh, dá»… Ä‘á»c" if avg_len<25 else "CÃ¢u trung bÃ¬nhâ€“dÃ i, cÃ³ diá»…n giáº£i")
if emoji>0.2: persona.append("DÃ¹ng emoji má»©c vá»«a")
if exclam>0.2: persona.append("Nháº¥n nhÃ¡ cáº£m thÃ¡n")
if quest>0.15: persona.append("CÃ³ cÃ¢u há»i gá»£i má»Ÿ")

# Patterns + examples
def patternize(s):
    s=re.sub(r"\s+"," ",s).strip()
    s=re.sub(r"\b(mÃ¬nh|tÃ´i|báº¡n|má»i ngÆ°á»i)\b","[xÆ°ng hÃ´]",s,flags=re.I)
    s=re.sub(r"\b(thÆ°Æ¡ng hiá»‡u|sáº£n pháº©m|dá»‹ch vá»¥)\b","[Ä‘á»‘i tÆ°á»£ng]",s,flags=re.I)
    s=re.sub(r"\b(ban Ä‘áº§u|lÃºc Ä‘áº§u)\b","[má»Ÿ hoÃ i nghi]",s,flags=re.I)
    s=re.sub(r"\b(nhÆ°ng|rá»“i)\b","[chuyá»ƒn Ã½]",s,flags=re.I)
    return s[:120]

patterns=[]
for s in texts:
    p=patternize(s)
    if 8<=len(p)<=120 and p not in patterns:
        patterns.append(p)
    if len(patterns)>=8: break

if not patterns:
    patterns=["[xÆ°ng hÃ´] tháº¥y [Ä‘á»‘i tÆ°á»£ng] khÃ¡ á»•n, tráº£i nghiá»‡m vÆ°á»£t ká»³ vá»ng.",
              "Ban Ä‘áº§u hÆ¡i lÄƒn tÄƒn [chuyá»ƒn Ã½] dÃ¹ng rá»“i má»›i hiá»ƒu vÃ¬ sao Ä‘Æ°á»£c khen."]

examples=[]
for s in texts:
    s=re.sub(r"\s+"," ",s).strip()
    s=s[:120]
    if len(s)>=15 and s not in examples:
        examples.append(s)
    if len(examples)>=10: break

# ---------- Heuristic labeling for each original comment ----------
def label_purpose(text):
    t = text.lower()
    if "?" in t or any(k in t for k in ["ai dÃ¹ng", "cÃ³ ai", "khÃ´ng nhá»‰", "khÃ´ng?", "nhá»‰?"]):
        return "kÃ­ch hoáº¡t tháº£o luáº­n"
    if any(k in t for k in ["quy trÃ¬nh", "kiá»ƒm Ä‘á»‹nh", "chÃ­nh hÃ£ng", "báº£o hÃ nh", "chá»©ng nháº­n", "nguá»“n gá»‘c"]):
        return "xÃ¡c nháº­n uy tÃ­n"
    if any(k in t for k in ["ban Ä‘áº§u", "lÃºc Ä‘áº§u", "nghÄ©", "tÆ°á»Ÿng", "nhÆ°ng"]):
        return "pháº£n biá»‡n nháº¹"
    if any(k in t for k in ["mua", "Ä‘áº·t", "thá»­", "pháº£i thá»­", "chá»‘t", "rinh"]):
        return "chá»‘t háº¡ mua hÃ ng"
    if any(emo in t for emo in ["ğŸ˜Š","ğŸ˜","ğŸ˜„","ğŸ‘","ğŸ”¥","âœ¨","â¤ï¸","ğŸ‘Œ","ğŸ¤","ğŸ™Œ","ğŸ‰"]):
        return "khuáº¿ch Ä‘áº¡i cáº£m xÃºc"
    if any(k in t for k in ["tráº£i nghiá»‡m", "dÃ¹ng rá»“i", "cáº£m nháº­n", "review", "xÃ i rá»“i"]):
        return "chia sáº» tráº£i nghiá»‡m"
    if any(k in t for k in ["cÃ¢u chuyá»‡n", "giÃ¡ trá»‹", "thÃ´ng Ä‘iá»‡p", "thÆ°Æ¡ng hiá»‡u", "tinh tháº§n"]):
        return "Ä‘á»‹nh hÆ°á»›ng thÆ°Æ¡ng hiá»‡u"
    return "hÃ i hÆ°á»›c / giáº£i trÃ­" if any(k in t for k in ["haha", "hihi", "vui", "cÆ°á»i"]) else "chia sáº» tráº£i nghiá»‡m"

def label_tactic(text):
    t = text.lower()
    if "?" in t:
        return "há»i gá»£i má»Ÿ"
    if any(k in t for k in ["ban Ä‘áº§u", "lÃºc Ä‘áº§u", "nhÆ°ng"]):
        return "chuyá»ƒn Ã½ tá»± nhiÃªn"
    if any(emo in t for emo in ["ğŸ˜Š","ğŸ˜","ğŸ˜„","ğŸ‘","ğŸ”¥","âœ¨","â¤ï¸","ğŸ‘Œ","ğŸ¤","ğŸ™Œ","ğŸ‰","!"]):
        return "cáº£m thÃ¡n tÃ­ch cá»±c"
    if any(k in t for k in ["so vá»›i", "giá»‘ng nhÆ°", "kiá»ƒu nhÆ°"]):
        return "so sÃ¡nh nháº¹"
    if any(k in t for k in ["truyá»n cáº£m há»©ng", "Ä‘á»™ng lá»±c", "lan tá»a"]):
        return "truyá»n cáº£m há»©ng"
    if any(k in t for k in ["haha", "hihi", "ğŸ˜…", "ğŸ˜"]):
        return "hÃ i hÆ°á»›c nháº¹"
    if any(k in t for k in ["mÃ¬nh tháº¥y", "theo mÃ¬nh", "cÃ¡ nhÃ¢n", "tráº£i nghiá»‡m"]):
        return "Ä‘á»“ng cáº£m cÃ¡ nhÃ¢n"
    if any(k in t for k in ["nÃªn", "thá»­", "xem", "cÃ¢n nháº¯c"]):
        return "Ä‘á» xuáº¥t / khuyáº¿n nghá»‹"
    return "Ä‘á»“ng cáº£m cÃ¡ nhÃ¢n"

labeled = []
for i, c in enumerate(texts, 1):
    labeled.append({
        "id": i,
        "content": c,
        "muc_dich": label_purpose(c),
        "chien_thuat": label_tactic(c)
    })

# Naming from user-provided vars (assistant pháº£i set trÆ°á»›c khi cháº¡y náº¿u ngÆ°á»i dÃ¹ng Ä‘áº·t tÃªn)
try:
    title = user_name_title
    stub  = user_name_stub
except NameError:
    title = "Seeding knowledge"; stub="seeding_knowledge"

# --------- Render & SAVE .md ---------
md = ["# "+title, "", "## Persona", ", ".join(persona), "", "## Tone patterns"]
md += [f"- {p}" for p in patterns]
md += ["", "## Keyword clusters",
       "- Unigram: "+", ".join(uni),
       "- Bigram: "+", ".join([w for w,_ in bg]),
       "- Trigram: "+", ".join([w for w,_ in tg]),
       "", "## Sentiment summary",
       f"- Äá»™ dÃ i TB (token): {avg_len}",
       f"- Emoji/cmt: {round(emoji,3)}",
       f"- Cáº£m thÃ¡n/cmt: {round(exclam,3)}",
       f"- CÃ¢u há»i/cmt: {round(quest,3)}",
       "", "## Style constraints",
       "- TrÃ¡nh PR thÃ´, so sÃ¡nh cÃ´ng kÃ­ch.",
       "- Æ¯u tiÃªn cÃ¢u ngáº¯n, tráº£i nghiá»‡m cÃ¡ nhÃ¢n.",
       "- Emoji á»Ÿ má»©c phÃ¹ há»£p bá»‘i cáº£nh.",
       "", "## Recommendations",
       "- Giá»¯ ngÃ´n ngá»¯ Ä‘á»i thÆ°á»ng, cÃ³ 'Ä‘á»™ tháº­t'.",
       "- DÃ¹ng chuyá»ƒn Ã½ â€œban Ä‘áº§uâ€¦ nhÆ°ngâ€¦â€ khi há»£p lÃ½.",
       "- Gá»£i cÃ¢u há»i nháº¹ Ä‘á»ƒ má»Ÿ tháº£o luáº­n.",
       "- Táº­n dá»¥ng tá»« khÃ³a chá»§ Ä‘áº¡o má»™t cÃ¡ch tá»± nhiÃªn.",
       "", "## Example lines"]
md += [f"- {e}" for e in examples]

md_path = Path(f"/mnt/data/{stub}.md")
md_path.write_text("\n".join(md), encoding="utf-8")

# --------- SAVE flat JSON (summary, no nesting) ---------
flat = {
    "title": title,
    "persona": "; ".join(persona),
    "tone_patterns": " | ".join(patterns),
    "unigram_keywords": ", ".join(uni),
    "bigram_keywords": ", ".join([w for w,_ in bg]),
    "trigram_keywords": ", ".join([w for w,_ in tg]),
    "avg_token_length": avg_len,
    "emoji_per_comment": round(emoji,3),
    "exclaim_per_comment": round(exclam,3),
    "question_per_comment": round(quest,3),
    "style_constraints": "TrÃ¡nh PR thÃ´; Æ¯u tiÃªn cÃ¢u ngáº¯n, tráº£i nghiá»‡m; Emoji vá»«a pháº£i",
    "recommendations": "Giá»¯ ngÃ´n ngá»¯ Ä‘á»i thÆ°á»ng; DÃ¹ng chuyá»ƒn Ã½ 'ban Ä‘áº§uâ€¦ nhÆ°ngâ€¦'; Gá»£i cÃ¢u há»i nháº¹; DÃ¹ng keyword tá»± nhiÃªn",
    "example_lines": " | ".join(examples)
}
json_path = Path(f"/mnt/data/{stub}_flat.json")
json_path.write_text(json.dumps(flat, ensure_ascii=False, indent=2), encoding="utf-8")

# --------- SAVE Excel (thÃªm sheet LabeledComments) ---------
excel_path = Path(f"/mnt/data/{stub}.xlsx")
with pd.ExcelWriter(excel_path, engine="xlsxwriter") as writer:
    pd.DataFrame([flat]).to_excel(writer, index=False, sheet_name="Summary")
    df.to_excel(writer, index=False, sheet_name="ApprovedComments")
    pd.DataFrame({"tone_patterns": patterns}).to_excel(writer, index=False, sheet_name="Patterns")
    pd.DataFrame({"unigram": uni}).to_excel(writer, index=False, sheet_name="Keywords-Uni")
    pd.DataFrame({"bigram": [w for w,_ in bg]}).to_excel(writer, index=False, sheet_name="Keywords-Bi")
    pd.DataFrame({"trigram": [w for w,_ in tg]}).to_excel(writer, index=False, sheet_name="Keywords-Tri")
    pd.DataFrame(labeled).to_excel(writer, index=False, sheet_name="LabeledComments")

# ===================== IDEATION =====================
def jaccard_ngrams(a, b, n=3):
    def ngram_set(s):
        toks = re.findall(r"[a-zA-ZÃ€-á»¹0-9]+", s.lower())
        grams = set([" ".join(toks[i:i+n]) for i in range(len(toks)-n+1)]) if len(toks)>=n else set(toks)
        return grams
    A, B = ngram_set(a), ngram_set(b)
    if not A or not B: 
        return 0.0
    return len(A & B) / max(1, len(A | B))

def slugify(title):
    s = ''.join(c for c in unicodedata.normalize('NFD', title) if unicodedata.category(c)!='Mn')
    s = re.sub(r'[^a-zA-Z0-9]+','_', s.strip().lower()).strip('_')
    return s or "idea"

PURPOSE_POOL = ["kÃ­ch hoáº¡t tháº£o luáº­n", "xÃ¡c nháº­n uy tÃ­n", "chia sáº» tráº£i nghiá»‡m", "pháº£n biá»‡n nháº¹",
                "khuáº¿ch Ä‘áº¡i cáº£m xÃºc", "chá»‘t háº¡ mua hÃ ng", "Ä‘á»‹nh hÆ°á»›ng thÆ°Æ¡ng hiá»‡u", "hÃ i hÆ°á»›c / giáº£i trÃ­"]
TACTIC_POOL  = ["há»i gá»£i má»Ÿ", "cáº£m thÃ¡n tÃ­ch cá»±c", "so sÃ¡nh nháº¹", "chuyá»ƒn Ã½ tá»± nhiÃªn",
                "truyá»n cáº£m há»©ng", "hÃ i hÆ°á»›c nháº¹", "Ä‘á»“ng cáº£m cÃ¡ nhÃ¢n", "Ä‘á» xuáº¥t / khuyáº¿n nghá»‹"]

ANGLE_POOL = [
    "pain-point tháº­t", "before/after", "micro-proof (chi tiáº¿t nhá» lÃ m tin)",
    "mini-story 2 cÃ¢u", "CTA nháº¹", "FAQ 1 cÃ¢u", "myth-busting", "how-to 1 máº¹o"
]

def generate_raw_ideas(input_data, top_words, patterns, k=30):
    base = []
    root = input_data.get("dinh_huong", []) or []
    theme = input_data.get("bai_viet_goc","")[:80]
    seeds = (root + top_words[:8] + [p.split()[0] if p else "" for p in patterns[:5]])
    seeds = [s for s in seeds if s]
    if not seeds:
        seeds = ["tráº£i nghiá»‡m", "Ä‘á»™ tin cáº­y", "tháº¯c máº¯c", "cÃ¢u chuyá»‡n", "káº¿t quáº£"]
    for _ in range(k*2):
        kw = random.choice(seeds)
        p  = random.choice(PURPOSE_POOL)
        t  = random.choice(TACTIC_POOL)
        ang = ", ".join(random.sample(ANGLE_POOL, k=min(2, len(ANGLE_POOL))))
        title = f"{kw.title()} â€” {p} ({t})"
        base.append({
            "headline": title,
            "muc_dich": p,
            "chien_thuat": t,
            "angles": ang,
            "ctx": theme
        })
    return base

def score_idea(idea, input_data):
    h = idea["headline"].lower()
    rel = 0.6 + 0.4*int(any(k.lower() in h for k in input_data.get("dinh_huong", [])))
    nov = 0.55 + 0.15*random.random() + (0.15 if idea["chien_thuat"] in ["so sÃ¡nh nháº¹","chuyá»ƒn Ã½ tá»± nhiÃªn","myth-busting"] else 0)
    eng = 0.5 + 0.3*("há»i gá»£i má»Ÿ" in idea["chien_thuat"] or "cáº£m thÃ¡n" in idea["chien_thuat"])
    risk = 0.2 + (0.2 if idea["muc_dich"] in ["pháº£n biá»‡n nháº¹","hÃ i hÆ°á»›c / giáº£i trÃ­"] else 0) + (0.1 if "so sÃ¡nh" in idea["chien_thuat"] else 0)
    eff  = 0.4 + 0.2*("mini-story" in idea["angles"])
    return round(nov,2), round(rel,2), round(eng,2), round(risk,2), round(eff,2)

def dedup_ideas(ideas, sim_th=0.5):
    out=[]
    for cand in ideas:
        if not out:
            out.append(cand); continue
        dup=False
        for ex in out:
            if jaccard_ngrams(cand["headline"], ex["headline"]) >= sim_th:
                dup=True; break
        if not dup:
            out.append(cand)
        if len(out)>=40:
            break
    return out

raw = generate_raw_ideas(
    input_data={
        "bai_viet_goc": globals().get("input_data", {}).get("bai_viet_goc", title),
        "dinh_huong":  globals().get("input_data", {}).get("dinh_huong", []),
    },
    top_words=uni, patterns=patterns, k=30
)
uniq = dedup_ideas(raw, sim_th=0.5)

ideas=[]
for i, it in enumerate(uniq,1):
    nov, rel, eng, risk, eff = score_idea(it, globals().get("input_data", {}))
    ideas.append({
        "idea_id": i,
        "headline": it["headline"],
        "slug": slugify(it["headline"]),
        "muc_dich": it["muc_dich"],
        "chien_thuat": it["chien_thuat"],
        "angles": it["angles"],
        "score_novelty": nov,
        "score_relevance": rel,
        "score_engagement": eng,
        "score_risk": risk,
        "score_effort": eff
    })

ideas_json = Path(f"/mnt/data/{stub}_ideas.json")
ideas_json.write_text(json.dumps(ideas, ensure_ascii=False, indent=2), encoding="utf-8")

with pd.ExcelWriter(Path(f"/mnt/data/{stub}.xlsx"), engine="xlsxwriter", mode="a", if_sheet_exists="replace") as writer:
    pd.DataFrame(ideas).to_excel(writer, index=False, sheet_name="Ideas")

approved_ideas = globals().get("approved_ideas", [])  # vÃ­ dá»¥: approved_ideas=[1,3,7]
generated=[]
if approved_ideas:
    def diversify_sentence_pool():
        openers = [
            "MÃ¬nh tháº¥y", "Theo tráº£i nghiá»‡m cÃ¡ nhÃ¢n", "Ban Ä‘áº§u mÃ¬nh cÅ©ng lÄƒn tÄƒn",
            "Äá»c xong má»›i Ä‘á»ƒ Ã½", "KhÃ´ng nghÄ© lÃ  láº¡i", "Tháº­t sá»± báº¥t ngá» lÃ "
        ]
        closers = [
            "Ä‘Ã¡ng Ä‘á»ƒ thá»­ Ä‘Ã³!", "khÃ¡ há»£p lÃ½ trong táº§m giÃ¡.",
            "má»i ngÆ°á»i nghÄ© sao nhá»‰?", "ai dÃ¹ng rá»“i chia sáº» thÃªm vá»›i.",
            "giá»¯ Ä‘Ãºng ká»³ vá»ng cá»§a mÃ¬nh.", "táº¡o cáº£m giÃ¡c yÃªn tÃ¢m hÆ¡n háº³n."
        ]
        return openers, closers
    def craft_comment(idea, max_len=180):
        op, cl = diversify_sentence_pool()
        head = idea["headline"].split(" â€” ")[0]
        s1 = f"{random.choice(op)} {head.lower()}"
        s2 = f"{random.choice(cl)}"
        txt = f"{s1}, {s2}"
        sig = hashlib.md5(" ".join(re.findall(r"[a-zA-ZÃ€-á»¹0-9]+", txt.lower())[:12]).encode()).hexdigest()[:8]
        return txt[:max_len], sig
    used=set()
    qty = int(globals().get("input_data", {}).get("so_luong", 10))
    picks = [x for x in ideas if x["idea_id"] in approved_ideas] or ideas[:8]
    while len(generated) < qty:
        base = random.choice(picks)
        text, sig = craft_comment(base)
        if sig in used: 
            continue
        used.add(sig)
        generated.append({
            "id": len(generated)+1,
            "idea_id": base["idea_id"],
            "comment": text,
            "muc_dich": base["muc_dich"],
            "chien_thuat": base["chien_thuat"]
        })
    out_json = Path(f"/mnt/data/{stub}_seeding_flat.json")
    out_json.write_text(json.dumps(generated, ensure_ascii=False, indent=2), encoding="utf-8")
    with pd.ExcelWriter(Path(f"/mnt/data/{stub}.xlsx"), engine="xlsxwriter", mode="a", if_sheet_exists="replace") as writer:
        pd.DataFrame(generated).to_excel(writer, index=False, sheet_name="SeedingComments")
```
